{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Answer Evaluator Model Server on Google Colab\n",
        "\n",
        "This notebook hosts your fine-tuned Llama model as a Flask API server accessible from your React Native app.\n",
        "\n",
        "## Setup Steps:\n",
        "1. Upload your model to Hugging Face or Google Drive\n",
        "2. Run all cells in order\n",
        "3. Copy the ngrok URL and update your React Native app\n",
        "4. Test the endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## üì¶ Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch accelerate flask flask-cors pyngrok\n",
        "!pip install bitsandbytes  # For 4-bit quantization\n",
        "\n",
        "# Import libraries\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "## ü§ñ Load Your Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_loading"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"Sathvik19/Answer-Evaluator-Model\"  # Your HuggingFace model\n",
        "# OR if you want to load from local files:\n",
        "# MODEL_NAME = \"/content/drive/MyDrive/your_model_path\"\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "\n",
        "# Configure 4-bit quantization for memory efficiency\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"Model device: {model.device if hasattr(model, 'device') else 'distributed'}\")\n",
        "print(f\"Memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "helper_functions"
      },
      "source": [
        "## üõ†Ô∏è Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helpers"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt, max_new_tokens=200, temperature=0.7):\n",
        "    \"\"\"Generate response from the model\"\"\"\n",
        "    try:\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(\n",
        "            prompt, \n",
        "            return_tensors=\"pt\", \n",
        "            truncation=True, \n",
        "            max_length=1024\n",
        "        )\n",
        "        \n",
        "        # Move to GPU if available\n",
        "        inputs = {k: v.to(model.device if hasattr(model, 'device') else 'cuda') for k, v in inputs.items()}\n",
        "        \n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.1\n",
        "            )\n",
        "        \n",
        "        # Decode response\n",
        "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Remove the input prompt from response\n",
        "        if prompt in response_text:\n",
        "            response_text = response_text.replace(prompt, \"\").strip()\n",
        "        \n",
        "        return response_text\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Generation error: {e}\")\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "def extract_json_from_text(text):\n",
        "    \"\"\"Extract JSON from model response\"\"\"\n",
        "    try:\n",
        "        # Look for JSON pattern\n",
        "        json_pattern = r'\\{[^{}]*\\}'\n",
        "        matches = re.findall(json_pattern, text)\n",
        "        \n",
        "        for match in matches:\n",
        "            try:\n",
        "                # Try to parse as JSON\n",
        "                parsed = json.loads(match)\n",
        "                return parsed\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "        \n",
        "        # If no valid JSON found, try to extract values manually\n",
        "        mark_match = re.search(r'(?:mark|score)\\s*[:\\-]?\\s*(\\d+)', text, re.IGNORECASE)\n",
        "        mark = int(mark_match.group(1)) if mark_match else 0\n",
        "        \n",
        "        return {\n",
        "            \"markAwarded\": mark,\n",
        "            \"explanation\": text[:200] + \"...\" if len(text) > 200 else text\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"markAwarded\": 0,\n",
        "            \"explanation\": f\"Error parsing response: {str(e)}\"\n",
        "        }\n",
        "\n",
        "def extract_boolean_from_text(text):\n",
        "    \"\"\"Extract boolean response for question comparison\"\"\"\n",
        "    try:\n",
        "        # Look for JSON first\n",
        "        json_pattern = r'\\{[^{}]*\\}'\n",
        "        match = re.search(json_pattern, text)\n",
        "        \n",
        "        if match:\n",
        "            try:\n",
        "                parsed = json.loads(match.group())\n",
        "                return parsed.get(\"isSame\", False)\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "        \n",
        "        # Fallback to text analysis\n",
        "        text_lower = text.lower()\n",
        "        if any(word in text_lower for word in ['yes', 'true', 'same', 'identical']):\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flask_app"
      },
      "source": [
        "## üåê Flask API Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flask_server"
      },
      "outputs": [],
      "source": [
        "# Create Flask app\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for React Native\n",
        "\n",
        "@app.route('/', methods=['GET'])\n",
        "def home():\n",
        "    return jsonify({\n",
        "        \"status\": \"online\",\n",
        "        \"message\": \"Answer Evaluator API is running\",\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"endpoints\": [\n",
        "            \"/compare - POST - Compare student answer with model answer\",\n",
        "            \"/compare-questions - POST - Compare if two questions are the same\",\n",
        "            \"/health - GET - Health check\"\n",
        "        ]\n",
        "    })\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\n",
        "        \"status\": \"healthy\",\n",
        "        \"gpu_available\": torch.cuda.is_available(),\n",
        "        \"gpu_memory\": f\"{torch.cuda.memory_allocated() / 1024**3:.2f} GB\" if torch.cuda.is_available() else \"N/A\"\n",
        "    })\n",
        "\n",
        "@app.route('/compare', methods=['POST'])\n",
        "def compare_answers():\n",
        "    try:\n",
        "        data = request.json\n",
        "        student_answer = data.get('studentAnswer', '')\n",
        "        model_answer = data.get('modelAnswer', '')\n",
        "        max_mark = int(data.get('maxMark', 5))\n",
        "        \n",
        "        # Create evaluation prompt\n",
        "        prompt = f\"\"\"You are a fair and accurate exam evaluator. Compare the student's answer with the model answer and provide a score.\n",
        "\n",
        "EVALUATION GUIDELINES:\n",
        "- If answers are IDENTICAL or convey the EXACT SAME MEANING: Award FULL marks ({max_mark})\n",
        "- If answers cover the same concepts with similar depth: Award FULL marks ({max_mark})\n",
        "- If answers are substantially correct but missing minor details: Deduct 0-1 marks\n",
        "- If answers are partially correct but missing key concepts: Award partial marks\n",
        "- If answers are incorrect or completely different: Award 0 marks\n",
        "\n",
        "Model Answer: \"{model_answer}\"\n",
        "Student Answer: \"{student_answer}\"\n",
        "\n",
        "Evaluate out of {max_mark} marks and provide your response in this exact JSON format:\n",
        "{{\"markAwarded\": number, \"explanation\": \"string\"}}\n",
        "\n",
        "Response:\"\"\"\n",
        "        \n",
        "        # Generate response\n",
        "        response_text = generate_response(prompt, max_new_tokens=150, temperature=0.3)\n",
        "        \n",
        "        # Extract JSON\n",
        "        result = extract_json_from_text(response_text)\n",
        "        \n",
        "        # Ensure mark is within bounds\n",
        "        result['markAwarded'] = max(0, min(int(result.get('markAwarded', 0)), max_mark))\n",
        "        \n",
        "        return jsonify(result)\n",
        "    \n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            \"markAwarded\": 0,\n",
        "            \"explanation\": f\"Error processing request: {str(e)}\"\n",
        "        }), 500\n",
        "\n",
        "@app.route('/compare-questions', methods=['POST'])\n",
        "def compare_questions():\n",
        "    try:\n",
        "        data = request.json\n",
        "        student_question = data.get('studentQuestion', '')\n",
        "        model_question = data.get('modelQuestion', '')\n",
        "        \n",
        "        # Create comparison prompt\n",
        "        prompt = f\"\"\"Determine if these two questions are asking the same thing. Ignore:\n",
        "- Extra spaces\n",
        "- Minor punctuation differences (periods, commas)\n",
        "- Question numbering format (Q.8 vs Q. 8. vs Q8 vs 8.)\n",
        "- Case differences\n",
        "\n",
        "Question 1: \"{model_question}\"\n",
        "Question 2: \"{student_question}\"\n",
        "\n",
        "Respond with JSON in this exact format:\n",
        "{{\"isSame\": boolean, \"explanation\": \"string\"}}\n",
        "\n",
        "Response:\"\"\"\n",
        "        \n",
        "        # Generate response\n",
        "        response_text = generate_response(prompt, max_new_tokens=100, temperature=0.1)\n",
        "        \n",
        "        # Extract boolean result\n",
        "        is_same = extract_boolean_from_text(response_text)\n",
        "        \n",
        "        return jsonify({\n",
        "            \"isSame\": is_same,\n",
        "            \"explanation\": response_text[:200] + \"...\" if len(response_text) > 200 else response_text\n",
        "        })\n",
        "    \n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            \"isSame\": False,\n",
        "            \"explanation\": f\"Error processing request: {str(e)}\"\n",
        "        }), 500\n",
        "\n",
        "print(\"‚úÖ Flask app created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start_server"
      },
      "source": [
        "## üöÄ Start Server with ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "server_start"
      },
      "outputs": [],
      "source": [
        "# Set up ngrok (you might need to sign up for a free account at ngrok.com)\n",
        "# Uncomment and add your ngrok auth token if needed:\n",
        "# ngrok.set_auth_token(\"your_ngrok_auth_token_here\")\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"üåê Public URL: {public_url}\")\n",
        "print(f\"üì± Use this URL in your React Native app: {public_url}\")\n",
        "\n",
        "# Start Flask server in a thread\n",
        "def run_server():\n",
        "    app.run(port=5000, debug=False, use_reloader=False)\n",
        "\n",
        "server_thread = threading.Thread(target=run_server)\n",
        "server_thread.daemon = True\n",
        "server_thread.start()\n",
        "\n",
        "print(\"‚úÖ Server started!\")\n",
        "print(\"\\nüß™ Test endpoints:\")\n",
        "print(f\"Health: {public_url}/health\")\n",
        "print(f\"Compare: {public_url}/compare\")\n",
        "print(f\"Compare Questions: {public_url}/compare-questions\")\n",
        "\n",
        "# Keep the server running\n",
        "print(\"\\nüîÑ Server is running... Keep this cell running to maintain the server.\")\n",
        "print(\"üìã Copy the public URL above to use in your React Native app.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_endpoints"
      },
      "source": [
        "## üß™ Test the API Endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_api"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Get the public URL (replace with your actual ngrok URL)\n",
        "base_url = str(public_url)\n",
        "\n",
        "print(f\"Testing API at: {base_url}\")\n",
        "\n",
        "# Test 1: Health check\n",
        "try:\n",
        "    response = requests.get(f\"{base_url}/health\")\n",
        "    print(f\"\\n‚úÖ Health Check: {response.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Health Check Failed: {e}\")\n",
        "\n",
        "# Test 2: Answer comparison\n",
        "try:\n",
        "    test_data = {\n",
        "        \"studentAnswer\": \"The capital of France is Paris. It is a beautiful city.\",\n",
        "        \"modelAnswer\": \"Paris is the capital of France.\",\n",
        "        \"maxMark\": 5\n",
        "    }\n",
        "    \n",
        "    response = requests.post(f\"{base_url}/compare\", json=test_data)\n",
        "    print(f\"\\n‚úÖ Answer Comparison: {response.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Answer Comparison Failed: {e}\")\n",
        "\n",
        "# Test 3: Question comparison\n",
        "try:\n",
        "    test_data = {\n",
        "        \"studentQuestion\": \"What is the capital of France?\",\n",
        "        \"modelQuestion\": \"What is the capital city of France?\"\n",
        "    }\n",
        "    \n",
        "    response = requests.post(f\"{base_url}/compare-questions\", json=test_data)\n",
        "    print(f\"\\n‚úÖ Question Comparison: {response.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Question Comparison Failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keep_alive"
      },
      "source": [
        "## ‚è∞ Keep Server Alive\n",
        "\n",
        "Run this cell to keep the server running and monitor its status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "monitor"
      },
      "outputs": [],
      "source": [
        "# Keep the server alive and monitor\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(30)  # Check every 30 seconds\n",
        "        \n",
        "        # Health check\n",
        "        try:\n",
        "            response = requests.get(f\"{public_url}/health\", timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                print(f\"üìä Server Status: Online - {time.strftime('%H:%M:%S')}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Server Warning: HTTP {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Server Error: {e}\")\n",
        "            \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüõë Server monitoring stopped\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Monitoring error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
