# üöÄ Complete Colab Integration Guide

This guide will help you set up your fine-tuned Llama model on Google Colab and integrate it with your AiAE React Native app.

## üìã Overview

Your setup will have:
- **Google Colab**: Hosts your fine-tuned model with GPU support
- **ngrok**: Makes your Colab server publicly accessible
- **React Native App**: Calls your Colab server for answer evaluation

## üîß Step-by-Step Setup

### Step 1: Prepare Your Model on Colab

1. **Upload the Colab Notebook**
   - Upload `Answer_Evaluator_Colab_Server.ipynb` to Google Colab
   - Make sure to enable GPU runtime: Runtime > Change runtime type > Hardware accelerator: GPU

2. **Configure Your Model**
   - In the notebook, update the `MODEL_NAME` variable:
   ```python
   MODEL_NAME = "Sathvik19/Answer-Evaluator-Model"  # Your HuggingFace model
   ```

3. **Get ngrok Auth Token (Optional but Recommended)**
   - Go to [ngrok.com](https://ngrok.com) and sign up for a free account
   - Get your auth token from the dashboard
   - Add it to the notebook:
   ```python
   ngrok.set_auth_token("your_ngrok_auth_token_here")
   ```

### Step 2: Run the Colab Server

1. **Execute All Cells**
   - Run all cells in the Colab notebook in order
   - The model will load (takes 2-3 minutes)
   - The server will start and ngrok will create a public URL

2. **Copy the Public URL**
   - Look for output like: `üåê Public URL: https://abc123.ngrok.io`
   - **Copy this URL - you'll need it in your React Native app!**

3. **Test the Server**
   - The notebook includes test cells to verify everything works
   - You should see successful test results

### Step 3: Update Your React Native App

1. **Configure the Server URL**
   - Open `src/services/colabServerService.js`
   - Replace `YOUR_NGROK_URL_HERE` with your actual ngrok URL:
   ```javascript
   this.baseUrl = 'https://abc123.ngrok.io'; // Your actual URL from Colab
   ```

2. **Alternative: Set URL Dynamically**
   ```javascript
   import colabServerService from './src/services/colabServerService';
   
   // Somewhere in your app startup
   colabServerService.setServerUrl('https://abc123.ngrok.io');
   ```

### Step 4: Test the Integration

1. **Start Your React Native App**
   ```bash
   npm start
   # or
   yarn start
   ```

2. **Test Answer Evaluation**
   - Go through the normal flow in your app
   - Submit some answers for evaluation
   - Check the console logs for connection status

3. **Monitor Colab Server**
   - Keep the Colab notebook running
   - Watch the monitoring cell for server status
   - The server will show request logs

## üõ†Ô∏è Configuration Options

### Model Parameters (in Colab)

You can adjust these parameters in the notebook:

```python
# For answer evaluation
max_new_tokens=150  # Response length
temperature=0.3     # Creativity (lower = more deterministic)

# For question comparison  
max_new_tokens=100
temperature=0.1     # Very deterministic for yes/no answers
```

### Server Configuration (in React Native)

```javascript
// In colabServerService.js
timeout: 30000,  // 30 second timeout for model inference
```

## üîß Troubleshooting

### Common Issues:

**1. Model Loading Errors**
```
Error: Model not found
```
**Solution**: Make sure your model `Sathvik19/Answer-Evaluator-Model` is public on Hugging Face or provide proper authentication.

**2. Server Connection Failed**
```
Error: Network request failed
```
**Solutions**:
- Check if Colab is still running
- Verify the ngrok URL is correct
- Make sure your phone/emulator can access the internet

**3. GPU Memory Issues**
```
CUDA out of memory
```
**Solutions**:
- Use Colab Pro for better GPU
- Reduce batch size or model size
- Restart the Colab runtime

**4. ngrok Session Expired**
```
ngrok tunnel expired
```
**Solutions**:
- Restart the ngrok cell in Colab
- Update your React Native app with the new URL
- Consider getting an ngrok Pro account for persistent URLs

### Monitoring Server Health

Add this to your React Native app to check server status:

```javascript
// Test server connection
const testServer = async () => {
  try {
    const health = await colabServerService.checkHealth();
    console.log('Server status:', health);
  } catch (error) {
    console.error('Server unavailable:', error);
  }
};
```

## üìä Performance Tips

### Optimize Response Time
1. **Keep Colab Active**: Run the monitoring cell to prevent idle timeout
2. **Reduce Model Size**: Use quantization (already configured in notebook)
3. **Batch Requests**: Send multiple evaluations together if possible

### Cost Management
1. **Free Colab**: ~12 hours of continuous use
2. **Colab Pro**: Better GPU, longer sessions ($10/month)
3. **Alternative**: Deploy to Google Cloud Run or AWS Lambda for production

## üîí Security Considerations

### For Development
- ngrok URLs are publicly accessible but hard to guess
- Don't share your ngrok URL publicly
- Server has basic input validation

### For Production
- Use proper authentication (API keys)
- Deploy to a secure cloud provider
- Implement rate limiting
- Use HTTPS (ngrok provides this automatically)

## üö¶ Deployment Options

### Current Setup (Development)
- ‚úÖ Free (with Colab limits)
- ‚úÖ Easy to set up
- ‚ùå Manual server management
- ‚ùå Limited uptime

### Production Alternatives

**Google Cloud Run**
- Auto-scaling
- Pay per use
- Better reliability
- Requires Docker knowledge

**AWS Lambda**
- Serverless
- Very cost-effective
- Cold start delays
- Requires model optimization

**Hugging Face Inference Endpoints**
- Managed service
- Easy deployment
- Costs more for custom models
- Professional support

## üì± React Native Integration Summary

Your app now uses:
1. **colabServerService.js**: Communicates with your Colab server
2. **SubmitAnswerScreen.jsx**: Updated to use Colab server instead of Gemini
3. **Same UI/UX**: No changes to the user experience

### Key Benefits
- **Custom Model**: Uses your specifically trained evaluator
- **Better Performance**: Optimized for your use case
- **Cost Control**: Free with Colab, scalable for production
- **Full Control**: You own the evaluation logic

## üÜò Support

If you encounter issues:

1. **Check Colab Logs**: Look for errors in the notebook output
2. **Verify Network**: Test the ngrok URL in a browser
3. **Check React Native Logs**: Use React Native debugger
4. **Test Endpoints**: Use the test functions in both Colab and React Native

## üéØ Next Steps

Once everything works:
1. **Optimize Prompts**: Fine-tune evaluation prompts for better accuracy
2. **Add Logging**: Monitor evaluation quality and response times  
3. **Scale Up**: Consider production deployment options
4. **Backup Model**: Save checkpoints of your fine-tuned model

---

**üéâ You now have a complete AI-powered answer evaluation system using your own fine-tuned model!**